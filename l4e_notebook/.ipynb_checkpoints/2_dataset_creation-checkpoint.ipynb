{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Amazon Lookout for Equipment** - Getting started\n",
    "*Part 2 - Dataset creation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "---\n",
    "This repository is structured as follow:\n",
    "\n",
    "```sh\n",
    ". lookout-equipment-demo\n",
    "|\n",
    "├── data/\n",
    "|   ├── interim                          # Temporary intermediate data are stored here\n",
    "|   ├── processed                        # Finalized datasets are usually stored here\n",
    "|   |                                    # before they are sent to S3 to allow the\n",
    "|   |                                    # service to reach them\n",
    "|   └── raw                              # Immutable original data are stored here\n",
    "|\n",
    "├── getting_started/\n",
    "|   ├── 1_data_preparation.ipynb\n",
    "|   ├── 2_dataset_creation.ipynb         <<< THIS NOTEBOOK <<<\n",
    "|   ├── 3_model_training.ipynb\n",
    "|   ├── 4_model_evaluation.ipynb\n",
    "|   ├── 5_inference_scheduling.ipynb\n",
    "|   └── 6_cleanup.ipynb\n",
    "|\n",
    "└── utils/\n",
    "    └── lookout_equipment_utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook configuration update\n",
    "Amazon Lookout for Equipment being a very recent service, we need to make sure that we have access to the latest version of the AWS Python packages. If you see a `pip` dependency error, check that the `boto3` version is ok: if it's greater than 1.17.48 (the first version that includes the `lookoutequipment` API), you can discard this error and move forward with the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 version: 1.17.99 (should be >= 1.17.48 to include Lookout for Equipment API)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install --quiet --upgrade boto3 awscli aiobotocore botocore sagemaker tqdm\n",
    "\n",
    "import boto3\n",
    "print(f'boto3 version: {boto3.__version__} (should be >= 1.17.48 to include Lookout for Equipment API)')\n",
    "\n",
    "# Restart the current notebook to ensure we take into account the previous updates:\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper functions for managing Lookout for Equipment API calls:\n",
    "sys.path.append('../utils')\n",
    "import lookout_equipment_utils as lookout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA = os.path.join('..', 'data', 'processed', '97978334-97be-4e45-bf5d-84d9e535a601')\n",
    "TRAIN_DATA     = os.path.join(PROCESSED_DATA, 'training-data')\n",
    "\n",
    "ROLE_ARN       = sagemaker.get_execution_role()\n",
    "REGION_NAME    = boto3.session.Session().region_name\n",
    "DATASET_NAME   = config.DATASET_NAME\n",
    "BUCKET         = config.BUCKET\n",
    "PREFIX         = config.PREFIX_TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['centrifugal-pump']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of the directories from the training data \n",
    "# directory: each directory corresponds to a subsystem:\n",
    "components = []\n",
    "for root, dirs, files in os.walk(f'{TRAIN_DATA}'):\n",
    "    for subsystem in dirs:\n",
    "        if subsystem != '.ipynb_checkpoints':\n",
    "            components.append(subsystem)\n",
    "        \n",
    "components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to setup the schema of your dataset. In the cell below, we define `DATASET_COMPONENT_FIELDS_MAP`. `DATASET_COMPONENT_FIELDS_MAP` is a Python dictonary (hashmap). The key of each entry in the dictionary is the `Component` name, and the value of each entry is a list of column names. The column names must exactly match the header in your CSV files. The order of the column names also need to exactly match:\n",
    "\n",
    "```json\n",
    "DATASET_COMPONENT_FIELDS_MAP = {\n",
    "    \"Component1\": ['Timestamp', 'Tag1', 'Tag2',...],\n",
    "    \"Component2\": ['Timestamp', 'Tag1', 'Tag2',...]\n",
    "    ...\n",
    "    \"ComponentN\": ['Timestamp', 'Tag1', 'Tag2',...]\n",
    "}\n",
    "```\n",
    "\n",
    "We also need to make sure the component name **matches exactly** the name of the folder in S3 (everything is **case sensitive**). As an example, when creating the data schema for the example we are using here, we will build a the dictionary that will look like this:\n",
    "```json\n",
    "DATASET_COMPONENT_FIELDS_MAP = {\n",
    "    \"centrifugal-pump\": ['Timestamp', 'Sensor0', 'Sensor1',... , 'Sensor29']\n",
    "}\n",
    "```\n",
    "The following cell builds this map, then convert it into a JSON schema that follows the following format, which is ready to be processed by Lookout for Equipment:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Components\": [\n",
    "    {\n",
    "      \"ComponentName\": \"centrifugal-pump\",\n",
    "      \"Columns\": [\n",
    "        {\"Name\": \"Timestamp\", \"Type\": \"DATETIME\"},\n",
    "        {\"Name\": \"Sensor0\", \"Type\": \"DOUBLE\"},\n",
    "        {\"Name\": \"Sensor1\", \"Type\": \"DOUBLE\"},\n",
    "        {\"Name\": \"Sensor2\", \"Type\": \"DOUBLE\"},\n",
    "        {\"Name\": \"Sensor3\", \"Type\": \"DOUBLE\"},\n",
    "          \n",
    "        ...\n",
    "          \n",
    "        {\"Name\": \"Sensor29\", \"Type\": \"DOUBLE\"}\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COMPONENT_FIELDS_MAP = dict()\n",
    "for subsystem in components:\n",
    "    subsystem_tags = ['Timestamp']\n",
    "    for root, _, files in os.walk(f'{TRAIN_DATA}/{subsystem}'):\n",
    "        for file in files:\n",
    "            fname = os.path.join(root, file)\n",
    "            current_subsystem_df = pd.read_csv(fname, nrows=1)\n",
    "            subsystem_tags = subsystem_tags + current_subsystem_df.columns.tolist()[1:]\n",
    "\n",
    "        DATASET_COMPONENT_FIELDS_MAP.update({subsystem: subsystem_tags})\n",
    "        \n",
    "        \n",
    "lookout_dataset = lookout.LookoutEquipmentDataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    component_fields_map=DATASET_COMPONENT_FIELDS_MAP,\n",
    "    region_name=REGION_NAME,\n",
    "    access_role_arn=ROLE_ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to use the console, the following string would be the one to use to configure the **dataset schema**:\n",
    "\n",
    "![Dataset creation with schema](assets/dataset-schema.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Components': [{'Columns': [{'Name': 'Timestamp', 'Type': 'DATETIME'},\n",
      "                             {'Name': 'Sensor0', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor1', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor2', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor3', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor4', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor5', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor6', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor7', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor8', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor9', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor10', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor11', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor24', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor25', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor26', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor27', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor28', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor29', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor12', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor13', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor14', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor15', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor16', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor17', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor18', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor19', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor20', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor21', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor22', 'Type': 'DOUBLE'},\n",
      "                             {'Name': 'Sensor23', 'Type': 'DOUBLE'}],\n",
      "                 'ComponentName': 'centrifugal-pump'}]}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=5)\n",
    "pp.pprint(eval(lookout_dataset.dataset_schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "The following method encapsulate the [**CreateDataset**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_CreateDataset.html) API:\n",
    "\n",
    "```python\n",
    "lookout_client.create_dataset(\n",
    "    DatasetName=self.dataset_name,\n",
    "    DatasetSchema={\n",
    "        'InlineDataSchema': \"schema\"\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"97978334-97be-4e45-bf5d-84d9e535a601\" does not exist, creating it...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DatasetName': '97978334-97be-4e45-bf5d-84d9e535a601',\n",
       " 'DatasetArn': 'arn:aws:lookoutequipment:us-east-1:593512547852:dataset/97978334-97be-4e45-bf5d-84d9e535a601/52d6f9f9-cebe-4a15-bcca-3bea29e618a8',\n",
       " 'Status': 'CREATED',\n",
       " 'ResponseMetadata': {'RequestId': 'c75ef9a7-96a9-47a1-974c-ab9d6e547b86',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c75ef9a7-96a9-47a1-974c-ab9d6e547b86',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '218',\n",
       "   'date': 'Fri, 23 Jul 2021 01:51:06 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookout_dataset.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now created, but it is empty and ready to receive some timeseries data that we will ingest from the S3 location prepared in the previous notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dataset created](assets/dataset-created.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest data into a dataset\n",
    "---\n",
    "Let's double check the values of all the parameters that will be used to ingest some data into an existing Lookout for Equipment dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arn:aws:iam::593512547852:role/L4ESagemaker-SageMakerIamRole-RLE9HD9GRB46',\n",
       " 'l4e-sitewise-d7bf5fa0',\n",
       " '97978334-97be-4e45-bf5d-84d9e535a601/training-data/',\n",
       " '97978334-97be-4e45-bf5d-84d9e535a601')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROLE_ARN, BUCKET, PREFIX, DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the ingestion job in the Lookout for Equipment dataset: the following method encapsulates the [**StartDataIngestionJob**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_StartDataIngestionJob.html) API:\n",
    "\n",
    "```python\n",
    "lookout_client.start_data_ingestion_job(\n",
    "    DatasetName=DATASET_NAME,\n",
    "    RoleArn=ROLE_ARN, \n",
    "    IngestionInputConfiguration={ \n",
    "        'S3InputConfiguration': { \n",
    "            'Bucket': BUCKET,\n",
    "            'Prefix': PREFIX\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lookout_dataset.ingest_data(BUCKET, PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingestion is launched. With this amount of data (around 50 MB), it should take between less than 5 minutes:\n",
    "\n",
    "![dataset_schema](assets/dataset-ingestion-in-progress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following cell to monitor the ingestion process by calling the [**DescribeDataIngestionJob**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_DescribeDataIngestionJob.html) API every 60 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Polling Data Ingestion Status=====\n",
      "\n",
      "2021-07-23 01:51:51 | IN_PROGRESS\n",
      "2021-07-23 01:52:51 | IN_PROGRESS\n",
      "2021-07-23 01:53:51 | IN_PROGRESS\n",
      "2021-07-23 01:54:51 | IN_PROGRESS\n",
      "2021-07-23 01:55:51 | SUCCESS\n",
      "\n",
      "=====End of Polling Data Ingestion Status=====\n"
     ]
    }
   ],
   "source": [
    "# Get the ingestion job ID and status:\n",
    "data_ingestion_job_id = response['JobId']\n",
    "data_ingestion_status = response['Status']\n",
    "\n",
    "# Wait until ingestion completes:\n",
    "print(\"=====Polling Data Ingestion Status=====\\n\")\n",
    "lookout_client = lookout.get_client(region_name=REGION_NAME)\n",
    "print(str(pd.to_datetime(datetime.now()))[:19], \"|\", data_ingestion_status)\n",
    "\n",
    "while data_ingestion_status == 'IN_PROGRESS':\n",
    "    time.sleep(60)\n",
    "    describe_data_ingestion_job_response = lookout_client.describe_data_ingestion_job(JobId=data_ingestion_job_id)\n",
    "    data_ingestion_status = describe_data_ingestion_job_response['Status']\n",
    "    print(str(pd.to_datetime(datetime.now()))[:19], \"|\", data_ingestion_status)\n",
    "    \n",
    "print(\"\\n=====End of Polling Data Ingestion Status=====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case any issue arise, you can inspect the API response available as a JSON document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JobId': '92288628733de3630a014d30a7bc9cc6',\n",
       " 'DatasetArn': 'arn:aws:lookoutequipment:us-east-1:593512547852:dataset/97978334-97be-4e45-bf5d-84d9e535a601/52d6f9f9-cebe-4a15-bcca-3bea29e618a8',\n",
       " 'IngestionInputConfiguration': {'S3InputConfiguration': {'Bucket': 'l4e-sitewise-d7bf5fa0',\n",
       "   'Prefix': '97978334-97be-4e45-bf5d-84d9e535a601/training-data/'}},\n",
       " 'RoleArn': 'arn:aws:iam::593512547852:role/L4ESagemaker-SageMakerIamRole-RLE9HD9GRB46',\n",
       " 'CreatedAt': datetime.datetime(2021, 7, 23, 1, 51, 40, 252000, tzinfo=tzlocal()),\n",
       " 'Status': 'SUCCESS',\n",
       " 'ResponseMetadata': {'RequestId': 'da597933-299d-482b-b73c-df63690ac7ea',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'da597933-299d-482b-b73c-df63690ac7ea',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '476',\n",
       "   'date': 'Fri, 23 Jul 2021 01:57:22 GMT',\n",
       "   'connection': 'close'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookout_client.describe_data_ingestion_job(JobId=data_ingestion_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingestion should now be complete as can be seen in the console:\n",
    "\n",
    "![Ingestion done](assets/dataset-ingestion-done.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we created a **Lookout for Equipment dataset** and ingested the S3 data previously uploaded into this dataset. **Move now to the next notebook to train a model based on these data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
