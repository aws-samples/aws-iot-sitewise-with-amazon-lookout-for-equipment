{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722acb03",
   "metadata": {},
   "source": [
    "# **Amazon Lookout for Equipment** - Getting started\n",
    "*Part 4 - Model evaluation*\n",
    "\n",
    "## Initialization\n",
    "---\n",
    "This repository is structured as follow:\n",
    "\n",
    "```sh\n",
    ". lookout-equipment-demo\n",
    "|\n",
    "├── data/\n",
    "|   ├── interim                          # Temporary intermediate data are stored here\n",
    "|   ├── processed                        # Finalized datasets are usually stored here\n",
    "|   |                                    # before they are sent to S3 to allow the\n",
    "|   |                                    # service to reach them\n",
    "|   └── raw                              # Immutable original data are stored here\n",
    "|\n",
    "├── getting_started/\n",
    "|   ├── 1_data_preparation.ipynb\n",
    "|   ├── 2_dataset_creation.ipynb\n",
    "|   ├── 3_model_training.ipynb\n",
    "|   ├── 4_model_evaluation.ipynb         <<< THIS NOTEBOOK <<<\n",
    "|   ├── 5_inference_scheduling.ipynb\n",
    "|   └── 6_cleanup.ipynb\n",
    "|\n",
    "└── utils/\n",
    "    └── lookout_equipment_utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7800c0",
   "metadata": {},
   "source": [
    "### Notebook configuration update\n",
    "Amazon Lookout for Equipment being a very recent service, we need to make sure that we have access to the latest version of the AWS Python packages. If you see a `pip` dependency error, check that the `boto3` version is ok: if it's greater than 1.17.48 (the first version that includes the `lookoutequipment` API), you can discard this error and move forward with the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836bdf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "print(f'boto3 version: {boto3.__version__} (should be >= 1.17.48 to include Lookout for Equipment API)')\n",
    "\n",
    "# Restart the current notebook to ensure we take into account the previous updates:\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b95678",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Helper functions for managing Lookout for Equipment API calls:\n",
    "sys.path.append('../utils')\n",
    "import lookout_equipment_utils as lookout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47031ed4",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d27771",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME     = config.MODEL_NAME\n",
    "ASSET_ID       = config.ASSET_ID\n",
    "TMP_DATA       = os.path.join('..', 'data', 'interim', ASSET_ID)\n",
    "PROCESSED_DATA = os.path.join('..', 'data', 'processed', ASSET_ID)\n",
    "LABEL_DATA     = os.path.join(PROCESSED_DATA, 'label-data')\n",
    "TRAIN_DATA     = os.path.join(PROCESSED_DATA, 'training-data', 'centrifugal-pump')\n",
    "REGION_NAME    = boto3.session.Session().region_name\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('Solarize_Light2')\n",
    "plt.rcParams['lines.linewidth'] = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f8ebd",
   "metadata": {},
   "source": [
    "Based on the label time ranges, we will use the following time ranges:\n",
    "\n",
    "* **Train set:** 1st January 2019 - 31st July 2019: Lookout for Equipment needs at least 180 days of training data and this period contains a few labelled ranges with some anomalies.\n",
    "* **Evaluation set:** 1st August 2019 - 27th October 2019 *(this test set includes both normal and abnormal data to evaluate our model on)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring time ranges:\n",
    "training_start   = pd.to_datetime('2019-01-01 00:00:00')\n",
    "training_end     = pd.to_datetime('2019-07-31 00:00:00')\n",
    "evaluation_start = pd.to_datetime('2019-08-01 00:00:00')\n",
    "evaluation_end   = pd.to_datetime('2019-10-27 00:00:00')\n",
    "\n",
    "print(f'  Training period | from {training_start} to {training_end}')\n",
    "print(f'Evaluation period | from {evaluation_start} to {evaluation_end}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5603509",
   "metadata": {},
   "source": [
    "### Loading original datasets for visualization purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d93751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load all our original signals (they will be useful later on):\n",
    "all_tags_fname = os.path.join(TRAIN_DATA, 'sensors.csv')\n",
    "all_tags_df = pd.read_csv(all_tags_fname)\n",
    "all_tags_df['Timestamp'] = pd.to_datetime(all_tags_df['Timestamp'])\n",
    "all_tags_df = all_tags_df.set_index('Timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ffe38b",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd9a9a6",
   "metadata": {},
   "source": [
    "The [**DescribeModel**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_DescribeModel.html) API can be used to extract, among other things, the metrics associated to the trained model. Here are the different fields available when calling this API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed24c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookout_client = lookout.get_client(region_name=REGION_NAME)\n",
    "describe_model_response = lookout_client.describe_model(ModelName=MODEL_NAME)\n",
    "list(describe_model_response.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4b5b6",
   "metadata": {},
   "source": [
    "The `ModelMetrics` field above is a dictionnary that follows this format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    'labeled_ranges': [\n",
    "        {'start': '2019-08-08T00:00:00.000000', 'end': '2019-08-09T00:00:00.000000'},\n",
    "        {'start': '2019-08-18T00:00:00.000000', 'end': '2019-08-19T00:00:00.000000'},\n",
    "        {'start': '2019-08-28T00:00:00.000000', 'end': '2019-08-29T00:00:00.000000'},\n",
    "        {'start': '2019-09-07T00:00:00.000000', 'end': '2019-09-08T00:00:00.000000'},\n",
    "        {'start': '2019-09-17T00:00:00.000000', 'end': '2019-09-18T00:00:00.000000'},\n",
    "        {'start': '2019-09-27T00:00:00.000000', 'end': '2019-09-28T00:00:00.000000'},\n",
    "        {'start': '2019-10-07T00:00:00.000000', 'end': '2019-10-08T00:00:00.000000'},\n",
    "        {'start': '2019-10-17T00:00:00.000000', 'end': '2019-10-18T00:00:00.000000'}\n",
    "    ],\n",
    "    'labeled_event_metrics': {\n",
    "        'num_labeled': 8,\n",
    "        'num_identified': 8,\n",
    "        'total_warning_time_in_seconds': 668040.0\n",
    "    },\n",
    "    'predicted_ranges': [\n",
    "        {\n",
    "            'start': '2019-08-08T00:42:00.000000',\n",
    "            'end': '2019-08-08T01:48:00.000000',\n",
    "            'diagnostics': [\n",
    "                {'name': 'centrifugal-pump\\\\Sensor0', 'value': 0.05218326564181105},\n",
    "                {'name': 'centrifugal-pump\\\\Sensor1', 'value': 0.023636079094576},\n",
    "                {'name': 'centrifugal-pump\\\\Sensor2', 'value': 0.03825258734479793},\n",
    "                {'name': 'centrifugal-pump\\\\Sensor3', 'value': 0.023349531399873558},\n",
    "                \n",
    "                ...\n",
    "                \n",
    "                {'name': 'centrifugal-pump\\\\Sensor20', 'value': 0.04989340342761552},\n",
    "                {'name': 'centrifugal-pump\\\\Sensor21', 'value': 0.033976174168938014},\n",
    "                {'name': 'centrifugal-pump\\\\Sensor22', 'value': 0.046622167459421035},\n",
    "                {'name': 'centrifugal-pump\\\\Sensor23', 'value': 0.044698573526762944}\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        ...\n",
    "        \n",
    "    ],\n",
    "    'unknown_event_metrics': {\n",
    "        'num_identified': 8,\n",
    "        'total_duration_in_seconds': 4200.0\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The `labeled_ranges` contains the label provided as an input while the `predicted_ranges` contains all the predicted ranges where Lookout for Equipment detected an anomaly. Each predicted range contains a `diagnostics` field with a percentage associated to each sensor available in the dataset. During the training, Lookout for Equipment learns the relationship between the sensors that denotes a normal behavior. When this normal relationship is broken, the service considers that it detected an an anomalous event. It then proceeds with calculating which sensors are indicating that the asset is no longer operating normally. You can read this diagnostic as a feature importance output of the model: the percentage associated to a given sensor corresponds to the magnitude of impact (*importance*) this sensor has with regards to a given anomaly.\n",
    "\n",
    "Let's use the following utility function get these results into two dataframes (labeled and predicted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LookoutDiagnostics = lookout.LookoutEquipmentAnalysis(model_name=MODEL_NAME, tags_df=all_tags_df, region_name=REGION_NAME)\n",
    "LookoutDiagnostics.set_time_periods(evaluation_start, evaluation_end, training_start, training_end)\n",
    "predicted_ranges = LookoutDiagnostics.get_predictions()\n",
    "labels_fname = os.path.join(LABEL_DATA, 'labels.csv')\n",
    "labeled_range = LookoutDiagnostics.get_labels(labels_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a613895c",
   "metadata": {},
   "source": [
    "**Note:** the labeled range from the model Describe API, only provides any labelled data falling within the evaluation range. We use the original label data to get all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833e261",
   "metadata": {},
   "source": [
    "Let's now display one of the original signal and map both the labeled and the predicted ranges on the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e169ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load one of the original signal we looked at in the data preparation step:\n",
    "tag = 'Sensor0'\n",
    "tag_df = all_tags_df.loc[training_start:evaluation_end, [tag]]\n",
    "tag_df.columns = ['Value']\n",
    "\n",
    "# Plot all of that:\n",
    "fig, axes = lookout.plot_timeseries(\n",
    "    timeseries_df=tag_df, \n",
    "    tag_name=tag,\n",
    "    fig_width=20, \n",
    "    tag_split=evaluation_start, \n",
    "    labels_df=labeled_range,\n",
    "    predictions=predicted_ranges,\n",
    "    custom_grid=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819ad3c",
   "metadata": {},
   "source": [
    "## Unpacking event details\n",
    "---\n",
    "### Single event overview\n",
    "Each detected event have some detailed diagnostics stored in JSON format. Let's unpack the event details for the first large event and plot a similar bar chart than what the console provides:\n",
    "\n",
    "![Event details](assets/model-diagnostics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabf77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the event details:\n",
    "first_event_details = predicted_ranges.loc[1, 'diagnostics']\n",
    "first_event_details = pd.DataFrame(first_event_details).sort_values(by='value', ascending=False).reset_index(drop=True)\n",
    "first_event_details = first_event_details.sort_values(by='value')\n",
    "\n",
    "# We can then plot a horizontal bar chart:\n",
    "y_pos = np.arange(first_event_details.shape[0])\n",
    "values = list(first_event_details['value'])\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = plt.subplot(1,1,1)\n",
    "ax.barh(y_pos, first_event_details['value'], align='center')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(first_event_details['name'])\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "# Add the values in each bar:\n",
    "for i, v in enumerate(values):\n",
    "    ax.text(0.0005, i, f'{v*100:.2f}%', color='#FFFFFF', fontweight='bold', verticalalignment='center')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbfdf0d",
   "metadata": {},
   "source": [
    "### Grouping sensors by component\n",
    "The above bar chart is already a great help to pinpoint what might be going wrong with your asset. Let's load the initial tags description file we prepared in the first notebook and match the sensors with our initial components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540935c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_description_fname = os.path.join(TMP_DATA, 'tags_description.csv')\n",
    "tags_description_df = pd.read_csv(tags_description_fname)\n",
    "first_event_details[['asset', 'sensor']] = first_event_details['name'].str.split('\\\\', expand=True)\n",
    "component_diagnostics = pd.merge(first_event_details, tags_description_df, how='inner', left_on='sensor', right_on='Tag')[['name', 'value', 'Component']]\n",
    "component_diagnostics.sort_values(by='value', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e454db2",
   "metadata": {},
   "source": [
    "If we group the contribution of all sensors by component we end up seeing that the volute component has a 30% contribution to this particular event, while the other components are ranging from 16 to 19%: **time to give the volute a visit?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af136c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_diagnostics = component_diagnostics.groupby(by='Component').sum().sort_values(by='value')\n",
    "event_diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then plot a horizontal bar chart:\n",
    "y_pos = np.arange(event_diagnostics.shape[0])\n",
    "values = list(event_diagnostics['value'])\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax = plt.subplot(1,1,1)\n",
    "ax.barh(y_pos, event_diagnostics['value'], align='center')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(list(event_diagnostics.index))\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "# Add the values in each bar:\n",
    "for i, v in enumerate(values):\n",
    "    ax.text(0.005, i, f'{v*100:.2f}%', color='#FFFFFF', fontweight='bold', verticalalignment='center')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f10d7c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2170c55",
   "metadata": {},
   "source": [
    "In this notebook, we use the model created in part 3 of this notebook series and performed a few visualization and diagnostics on the results obtained. You can now move forward to the next step to the **inference scheduling notebook** where we will start the model, feed it some new data and catch the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
