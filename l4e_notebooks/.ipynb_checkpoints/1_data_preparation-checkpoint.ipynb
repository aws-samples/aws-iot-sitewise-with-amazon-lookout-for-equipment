{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Amazon Lookout for Equipment**\n",
    "*Part 1 - Data preparation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook configuration update\n",
    "Let's make sure that we have access to the latest version of the AWS Python packages. If you see a `pip` dependency error, check that the `boto3` version is ok: if it's greater than 1.17.48 (the first version that includes the `lookoutequipment` API), you can discard this error and move forward with the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "print(f'boto3 version: {boto3.__version__} (should be >= 1.17.48 to include Lookout for Equipment API)')\n",
    "\n",
    "# Restart the current notebook to ensure we take into account the previous updates:\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from botocore.client import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the region and the availability of Amazon Lookout for Equipment in this region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION_NAME = boto3.session.Session().region_name\n",
    "\n",
    "ssm_client = boto3.client('ssm')\n",
    "response = ssm_client.get_parameters_by_path(\n",
    "    Path='/aws/service/global-infrastructure/services/lookoutequipment/regions',\n",
    ")\n",
    "\n",
    "available_regions = [r['Value'] for r in response['Parameters']]\n",
    "if REGION_NAME not in available_regions:\n",
    "    raise Exception(f'Amazon Lookout for Equipment is only available in {available_regions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Let's first check if the bucket name is defined, if it exists and if we have access to it from this notebook. If this notebook does not have access to the S3 bucket, you will have to update its permission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "BUCKET           = config.BUCKET\n",
    "ASSET_ID         = config.ASSET_ID\n",
    "\n",
    "PREFIX_TRAINING  = f'{ASSET_ID}/training-data/'\n",
    "PREFIX_LABEL     = f'{ASSET_ID}/label-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET == '<<YOUR_BUCKET>>':\n",
    "    raise Exception('Please update your Amazon S3 bucket name in the config.py file located at the root of this repository and restart the kernel for this notebook.')\n",
    "    \n",
    "else:\n",
    "    # Check access to the configured bucket:\n",
    "    try:\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        s3_resource.meta.client.head_bucket(Bucket=BUCKET)\n",
    "        print(f'Bucket \"{BUCKET}\" exists')\n",
    "        \n",
    "    # Expose error reason:\n",
    "    except ClientError as error:\n",
    "        error_code = int(error.response['Error']['Code'])\n",
    "        if error_code == 403:\n",
    "            raise Exception(f'Bucket \"{BUCKET}\" is private: access is forbidden!')\n",
    "            \n",
    "        elif error_code == 404:\n",
    "            raise Exception(f'Bucket \"{BUCKET}\" does not exist!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA       = os.path.join('..', 'data', 'raw', ASSET_ID)\n",
    "TMP_DATA       = os.path.join('..', 'data', 'interim', ASSET_ID)\n",
    "PROCESSED_DATA = os.path.join('..', 'data', 'processed', ASSET_ID)\n",
    "LABEL_DATA     = os.path.join(PROCESSED_DATA, 'label-data')\n",
    "TRAIN_DATA     = os.path.join(PROCESSED_DATA, 'training-data')\n",
    "INFERENCE_DATA = os.path.join(PROCESSED_DATA, 'inference-data')\n",
    "\n",
    "os.makedirs(TMP_DATA,         exist_ok=True)\n",
    "os.makedirs(RAW_DATA,         exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA,   exist_ok=True)\n",
    "os.makedirs(LABEL_DATA,       exist_ok=True)\n",
    "os.makedirs(TRAIN_DATA,       exist_ok=True)\n",
    "os.makedirs(INFERENCE_DATA,   exist_ok=True)\n",
    "\n",
    "ORIGINAL_DATA = f's3://lookoutforequipmentbucket-{REGION_NAME}/datasets/getting-started/lookout-equipment-sdk-5min.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data\n",
    "---\n",
    "Downloading and unzipping the getting started dataset locally on this instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exists = os.path.exists(os.path.join(TMP_DATA, 'sensors-data', 'impeller', 'component2_file1.csv'))\n",
    "raw_data_exists = os.path.exists(os.path.join(RAW_DATA, 'lookout-equipment.zip'))\n",
    "\n",
    "if data_exists:\n",
    "    print('Dataset already available locally, nothing to do.')\n",
    "    print(f'Dataset is available in {TMP_DATA}.')\n",
    "    \n",
    "else:\n",
    "    if not raw_data_exists:\n",
    "        print('Raw data not found, downloading it')\n",
    "        !aws s3 cp $ORIGINAL_DATA $RAW_DATA/lookout-equipment.zip\n",
    "        \n",
    "    print('Unzipping raw data...')\n",
    "    !unzip $RAW_DATA/lookout-equipment.zip -d $TMP_DATA\n",
    "    print(f'Done: dataset now available in {TMP_DATA}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing time series data\n",
    "---\n",
    "The time series data are available in the `sensors-data` directory. The industrial asset we are looking at is a [centrifugal pump](https://en.wikipedia.org/wiki/Centrifugal_pump). Such a pump is used to move a fluid by transfering the rotational energy provided by a motor to hydrodynamic energy:\n",
    "\n",
    "<img src=\"assets/centrifugal_pump_annotated.png\" alt=\"Centrifugal pump\" style=\"width: 658px\"/>\n",
    "\n",
    "<div style=\"text-align: center\"><i>Warman centrifugal pump in a coal preparation plant application</i>, by Bernard S. Janse, licensed under <a href=\"https://creativecommons.org/licenses/by/2.5/deed.fr\">CC BY 2.5</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a pump such as the one displayed in the photo above, the fluid enters at its axis (the black pipe arriving at the \"eye\" of the impeller. Measurements can be taken around the four main components of the centrifugal pump:\n",
    "* The **impeller** (hidden into the round white casing above): this component consists of a series of curved vanes (blades)\n",
    "* The drive **shaft** arriving at the impeller axis (the \"eye\")\n",
    "* The **motor** connected to the impeller by the drive shaft (on the other end of the black pipe above)\n",
    "* The **volute** chamber, offseted on the right compared to the impeller axis: this creates a curved funnel win a decreasing cross-section area towards the pump outlet (at the top of the white pipe above)\n",
    "\n",
    "In the dataset provided, other sensors not located on one of these component are positionned at the **pump** level.\n",
    "\n",
    "**Let's load the content of each CSV file (we have one per component) and build a single CSV file with all the sensors:** we will obtain a dataset with 10 months of data (spanning from `2019-01-01` to `2019-10-27`) for 30 sensors (`Sensor0` to `Sensor29`) with a 1-minute sampling rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Loops through each subfolder of the original dataset:\n",
    "sensor_df_list = []\n",
    "tags_description_dict = dict()\n",
    "for root, dirs, files in os.walk(os.path.join(TMP_DATA, 'sensors-data')):\n",
    "    # Reads each file and set the first column as an index:\n",
    "    for f in files:\n",
    "        print('Processing:', os.path.join(root, f))\n",
    "        df = pd.read_csv(os.path.join(root, f))\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df = df.set_index('Timestamp')\n",
    "        sensor_df_list.append(df)\n",
    "        \n",
    "        component = root.split('/')[-1]\n",
    "        current_sensors = df.columns.tolist()\n",
    "        current_sensors = dict(zip(current_sensors, [component] * len(current_sensors)))\n",
    "        tags_description_dict = {**tags_description_dict, **current_sensors}\n",
    "        \n",
    "# Concatenate into a single dataframe:\n",
    "equipment_df = pd.concat(sensor_df_list, axis='columns')\n",
    "equipment_df = equipment_df.reset_index()\n",
    "equipment_df['Timestamp'] = pd.to_datetime(equipment_df['Timestamp'])\n",
    "equipment_df = equipment_df[[\n",
    "    'Timestamp', 'Sensor0', 'Sensor1', 'Sensor2', 'Sensor3', 'Sensor4',\n",
    "    'Sensor5', 'Sensor6', 'Sensor7', 'Sensor8', 'Sensor9', 'Sensor10',\n",
    "    'Sensor11', 'Sensor24', 'Sensor25', 'Sensor26', 'Sensor27', 'Sensor28',\n",
    "    'Sensor29', 'Sensor12', 'Sensor13', 'Sensor14', 'Sensor15', 'Sensor16',\n",
    "    'Sensor17', 'Sensor18', 'Sensor19', 'Sensor20', 'Sensor21', 'Sensor22',\n",
    "    'Sensor23'\n",
    "]]\n",
    "\n",
    "# Register a component for each sensor:\n",
    "tags_description_df = pd.DataFrame.from_dict(tags_description_dict, orient='index')\n",
    "tags_description_df = tags_description_df.reset_index()\n",
    "tags_description_df.columns = ['Tag', 'Component']\n",
    "\n",
    "print(equipment_df.shape)\n",
    "equipment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipment_df['Timestamp'] = pd.to_datetime(equipment_df['Timestamp'])\n",
    "equipment_df = equipment_df.set_index('Timestamp')\n",
    "equipment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "os.makedirs(os.path.join(TRAIN_DATA, 'centrifugal-pump'), exist_ok=True)\n",
    "equipment_fname = os.path.join(TRAIN_DATA, 'centrifugal-pump', 'sensors.csv')\n",
    "equipment_df.to_csv(equipment_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also persist the tags description file as it will be useful when analyzing the model results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_description_fname = os.path.join(TMP_DATA, 'tags_description.csv')\n",
    "tags_description_df.to_csv(tags_description_fname, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label data\n",
    "---\n",
    "This dataset contains synthetically generated anomalies over different periods of time. Labels are stored as time ranges with a start and end timestamp. Each label is a period of time where we know some anomalous behavior happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_fname = os.path.join(TMP_DATA, 'label-data', 'labels.csv')\n",
    "labels_df = pd.read_csv(label_fname, header=None)\n",
    "labels_df.to_csv(os.path.join(PROCESSED_DATA, 'label-data', 'labels.csv'), index=None, header=None)\n",
    "labels_df.columns = ['start', 'end']\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to Amazon S3\n",
    "---\n",
    "Let's now load our training data and labels to Amazon S3, so that Lookout for Equipment can access them to train and evaluate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3_path = f's3://{BUCKET}/{PREFIX_TRAINING}centrifugal-pump/sensors.csv'\n",
    "!aws s3 cp $equipment_fname $train_s3_path\n",
    "\n",
    "label_s3_path = f's3://{BUCKET}/{PREFIX_LABEL}labels.csv'\n",
    "!aws s3 cp $label_fname $label_s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "\n",
    "### Enabling notifications in Jupyter Notebook\n",
    "\n",
    "**Notify** is a Jupyter Notebook extension that notifies the user once a long-running cell has finished executing. It does so through browser notification. When you’ll run the below cell for the first time, your browser will ask you to allow notifications in your notebook. You should press ‘Yes.’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyternotify --quiet\n",
    "\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for visualizing markdowns programatically\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\n",
    "'''\n",
    "<span style=\"color:green\"><span style=\"font-size:50px\">**Success!**</span></span>\n",
    "<br/>\n",
    "In this notebook, you downloaded the getting started dataset and prepared it for ingestion in Amazon Lookout for Equipment.\n",
    "\n",
    "You also had a quick overview of the dataset with basic timeseries visualization.\n",
    "\n",
    "You uploaded the training time series data and the anomaly labels to Amazon S3: in the next notebook of this getting started, you will be acquainted with the Amazon Lookout for Equipment API to create your first dataset. \n",
    "\n",
    "Move on to the next notebook.\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
