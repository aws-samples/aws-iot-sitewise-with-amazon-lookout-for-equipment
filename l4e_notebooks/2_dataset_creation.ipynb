{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a4727e",
   "metadata": {},
   "source": [
    "# **Amazon Lookout for Equipment**\n",
    "*Part 2 - Dataset creation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd01677",
   "metadata": {},
   "source": [
    "### Notebook configuration update\n",
    "Let's make sure that we have access to the latest version of the AWS Python packages. If you see a `pip` dependency error, check that the `boto3` version is ok: if it's greater than 1.17.48 (the first version that includes the `lookoutequipment` API), you can discard this error and move forward with the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5e499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "boto3 version: 1.18.28 (should be >= 1.17.48 to include Lookout for Equipment API)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --quiet sagemaker==2.46.1 tqdm==4.62.1\n",
    "\n",
    "import boto3\n",
    "print(f'boto3 version: {boto3.__version__} (should be >= 1.17.48 to include Lookout for Equipment API)')\n",
    "\n",
    "# Restart the current notebook to ensure we take into account the previous updates:\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095c9b3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef28b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper functions for managing Lookout for Equipment API calls:\n",
    "sys.path.append('../utils')\n",
    "import lookout_equipment_utils as lookout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec139956",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_ARN       = sagemaker.get_execution_role()\n",
    "REGION_NAME    = boto3.session.Session().region_name\n",
    "ASSET_ID       = config.ASSET_ID\n",
    "DATASET_NAME   = config.DATASET_NAME\n",
    "BUCKET         = config.BUCKET\n",
    "PREFIX         = config.PREFIX_TRAINING\n",
    "\n",
    "PROCESSED_DATA = os.path.join('..', 'data', 'processed', ASSET_ID)\n",
    "TRAIN_DATA     = os.path.join(PROCESSED_DATA, 'training-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d04cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['centrifugal-pump']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of the directories from the training data \n",
    "# directory: each directory corresponds to a subsystem:\n",
    "components = []\n",
    "for root, dirs, files in os.walk(f'{TRAIN_DATA}'):\n",
    "    for subsystem in dirs:\n",
    "        if subsystem != '.ipynb_checkpoints':\n",
    "            components.append(subsystem)\n",
    "        \n",
    "components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6637e",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700efb03",
   "metadata": {},
   "source": [
    "### Create data schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb4b984",
   "metadata": {},
   "source": [
    "First we need to setup the schema of your dataset. In the cell below, we define `DATASET_COMPONENT_FIELDS_MAP`. `DATASET_COMPONENT_FIELDS_MAP` is a Python dictonary (hashmap). The key of each entry in the dictionary is the `Component` name, and the value of each entry is a list of column names. The column names must exactly match the header in your CSV files. The order of the column names also need to exactly match:\n",
    "\n",
    "```json\n",
    "DATASET_COMPONENT_FIELDS_MAP = {\n",
    "    \"Component1\": ['Timestamp', 'Tag1', 'Tag2',...],\n",
    "    \"Component2\": ['Timestamp', 'Tag1', 'Tag2',...]\n",
    "    ...\n",
    "    \"ComponentN\": ['Timestamp', 'Tag1', 'Tag2',...]\n",
    "}\n",
    "```\n",
    "\n",
    "We also need to make sure the component name **matches exactly** the name of the folder in S3 (everything is **case sensitive**). As an example, when creating the data schema for the example we are using here, we will build a the dictionary that will look like this:\n",
    "```json\n",
    "DATASET_COMPONENT_FIELDS_MAP = {\n",
    "    \"centrifugal-pump\": ['Timestamp', 'Sensor0', 'Sensor1',... , 'Sensor29']\n",
    "}\n",
    "```\n",
    "The following cell builds this map, then convert it into a JSON schema that follows the following format, which is ready to be processed by Lookout for Equipment:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Components\": [\n",
    "    {\n",
    "      \"ComponentName\": \"centrifugal-pump\",\n",
    "      \"Columns\": [\n",
    "        {\"Name\": \"Timestamp\", \"Type\": \"DATETIME\"},\n",
    "        {\"Name\": \"Sensor0\", \"Type\": \"DOUBLE\"},\n",
    "        {\"Name\": \"Sensor1\", \"Type\": \"DOUBLE\"},\n",
    "        {\"Name\": \"Sensor2\", \"Type\": \"DOUBLE\"},\n",
    "        {\"Name\": \"Sensor3\", \"Type\": \"DOUBLE\"},\n",
    "          \n",
    "        ...\n",
    "          \n",
    "        {\"Name\": \"Sensor29\", \"Type\": \"DOUBLE\"}\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3897ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COMPONENT_FIELDS_MAP = dict()\n",
    "for subsystem in components:\n",
    "    subsystem_tags = ['Timestamp']\n",
    "    for root, _, files in os.walk(f'{TRAIN_DATA}/{subsystem}'):\n",
    "        for file in files:\n",
    "            fname = os.path.join(root, file)\n",
    "            current_subsystem_df = pd.read_csv(fname, nrows=1)\n",
    "            subsystem_tags = subsystem_tags + current_subsystem_df.columns.tolist()[1:]\n",
    "\n",
    "        DATASET_COMPONENT_FIELDS_MAP.update({subsystem: subsystem_tags})\n",
    "        \n",
    "        \n",
    "lookout_dataset = lookout.LookoutEquipmentDataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    component_fields_map=DATASET_COMPONENT_FIELDS_MAP,\n",
    "    region_name=REGION_NAME,\n",
    "    access_role_arn=ROLE_ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0a843",
   "metadata": {},
   "source": [
    "If you wanted to use the console, the following string would be the one to use to configure the **dataset schema**:\n",
    "\n",
    "![Dataset creation with schema](assets/dataset-schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3190a",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "The following method encapsulate the [**CreateDataset**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_CreateDataset.html) API:\n",
    "\n",
    "```python\n",
    "lookout_client.create_dataset(\n",
    "    DatasetName=self.dataset_name,\n",
    "    DatasetSchema={\n",
    "        'InlineDataSchema': \"schema\"\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642adebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"f866f7a1-767b-4eca-a340-fa4ea4bdbba6\" already exists and can be used to ingest data or train a model.\n"
     ]
    }
   ],
   "source": [
    "lookout_dataset.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4964129",
   "metadata": {},
   "source": [
    "The dataset is now created, but it is empty and ready to receive some timeseries data that we will ingest from the S3 location prepared in the previous notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c1cdf5",
   "metadata": {},
   "source": [
    "![Dataset created](assets/dataset-created.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49aa5f9",
   "metadata": {},
   "source": [
    "## Ingest data into a dataset\n",
    "---\n",
    "Let's double check the values of all the parameters that will be used to ingest some data into an existing Lookout for Equipment dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c39da942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arn:aws:iam::631071447677:role/LookoutForEquipmentSitewisePipeli-SageMakerIamRole-1G00GVH0Y5G4D',\n",
       " 'l4ef866f-asset1-train-inference',\n",
       " 'f866f7a1-767b-4eca-a340-fa4ea4bdbba6/training-data/',\n",
       " 'f866f7a1-767b-4eca-a340-fa4ea4bdbba6')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROLE_ARN, BUCKET, PREFIX, DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3da83",
   "metadata": {},
   "source": [
    "Launch the ingestion job in the Lookout for Equipment dataset: the following method encapsulates the [**StartDataIngestionJob**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_StartDataIngestionJob.html) API:\n",
    "\n",
    "```python\n",
    "lookout_client.start_data_ingestion_job(\n",
    "    DatasetName=DATASET_NAME,\n",
    "    RoleArn=ROLE_ARN, \n",
    "    IngestionInputConfiguration={ \n",
    "        'S3InputConfiguration': { \n",
    "            'Bucket': BUCKET,\n",
    "            'Prefix': PREFIX\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7baba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lookout_dataset.ingest_data(BUCKET, PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea31a9",
   "metadata": {},
   "source": [
    "The ingestion is launched. With this amount of data (around 50 MB), it should take between less than 5 minutes:\n",
    "\n",
    "![dataset_schema](assets/dataset-ingestion-in-progress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8edee",
   "metadata": {},
   "source": [
    "We use the following cell to monitor the ingestion process by calling the [**DescribeDataIngestionJob**](https://docs.aws.amazon.com/lookout-for-equipment/latest/ug/API_DescribeDataIngestionJob.html) API every 60 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65e0cb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Polling Data Ingestion Status=====\n",
      "\n",
      "2021-09-01 14:28:40 | IN_PROGRESS\n",
      "2021-09-01 14:29:40 | IN_PROGRESS\n",
      "2021-09-01 14:30:40 | IN_PROGRESS\n",
      "2021-09-01 14:31:40 | IN_PROGRESS\n",
      "2021-09-01 14:32:40 | SUCCESS\n",
      "\n",
      "=====End of Polling Data Ingestion Status=====\n"
     ]
    }
   ],
   "source": [
    "# Get the ingestion job ID and status:\n",
    "data_ingestion_job_id = response['JobId']\n",
    "data_ingestion_status = response['Status']\n",
    "\n",
    "# Wait until ingestion completes:\n",
    "print(\"=====Polling Data Ingestion Status=====\\n\")\n",
    "lookout_client = lookout.get_client(region_name=REGION_NAME)\n",
    "print(str(pd.to_datetime(datetime.now()))[:19], \"|\", data_ingestion_status)\n",
    "\n",
    "while data_ingestion_status == 'IN_PROGRESS':\n",
    "    time.sleep(60)\n",
    "    describe_data_ingestion_job_response = lookout_client.describe_data_ingestion_job(JobId=data_ingestion_job_id)\n",
    "    data_ingestion_status = describe_data_ingestion_job_response['Status']\n",
    "    print(str(pd.to_datetime(datetime.now()))[:19], \"|\", data_ingestion_status)\n",
    "    \n",
    "print(\"\\n=====End of Polling Data Ingestion Status=====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe33f50e",
   "metadata": {},
   "source": [
    "In case any issue arise, you can inspect the API response available as a JSON document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf06e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JobId': '0677c855c0409bb3f350d45d967c2beb',\n",
       " 'DatasetArn': 'arn:aws:lookoutequipment:us-east-1:631071447677:dataset/f866f7a1-767b-4eca-a340-fa4ea4bdbba6/84d5640e-09f1-44a7-b99f-ae3bd993508e',\n",
       " 'IngestionInputConfiguration': {'S3InputConfiguration': {'Bucket': 'l4ef866f-asset1-train-inference',\n",
       "   'Prefix': 'f866f7a1-767b-4eca-a340-fa4ea4bdbba6/training-data/'}},\n",
       " 'RoleArn': 'arn:aws:iam::631071447677:role/LookoutForEquipmentSitewisePipeli-SageMakerIamRole-1G00GVH0Y5G4D',\n",
       " 'CreatedAt': datetime.datetime(2021, 9, 1, 14, 28, 40, 357000, tzinfo=tzlocal()),\n",
       " 'Status': 'SUCCESS',\n",
       " 'ResponseMetadata': {'RequestId': '06d56728-5cb2-4a91-a7b6-7d4d11fc6ce0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '06d56728-5cb2-4a91-a7b6-7d4d11fc6ce0',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '508',\n",
       "   'date': 'Wed, 01 Sep 2021 14:32:40 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookout_client.describe_data_ingestion_job(JobId=data_ingestion_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be122266",
   "metadata": {},
   "source": [
    "The ingestion should now be complete as can be seen in the console:\n",
    "\n",
    "![Ingestion done](assets/dataset-ingestion-done.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53250e26",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20b95add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "<span style=\"color:green\"><span style=\"font-size:50px\">**Success!**</span></span>\n",
       "<br/>\n",
       "In this notebook, we created a **Lookout for Equipment dataset** and ingested the S3 data previously uploaded into this dataset. **Move now to the next notebook to train a model based on these data.**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Needed for visualizing markdowns programatically\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\n",
    "'''\n",
    "<span style=\"color:green\"><span style=\"font-size:50px\">**Success!**</span></span>\n",
    "<br/>\n",
    "In this notebook, we created a **Lookout for Equipment dataset** and ingested the S3 data previously uploaded into this dataset. **Move now to the next notebook to train a model based on these data.**\n",
    "'''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
